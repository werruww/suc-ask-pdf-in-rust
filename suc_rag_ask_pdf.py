# -*- coding: utf-8 -*-
"""suc_rag_ask_pdf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16gdVFUWb6iKPK_guYjLKdVRyzRvGYzX9
"""







"""### https://github.com/yarenty/kowalski

A Rust-based agent for interacting with Ollama models
"""





!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

import os
os.environ['PATH'] += ":$HOME/.cargo/bin"

# تثبيت Rust
!curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y

# تحديث متغير PATH لجعله متاحًا للخلايا اللاحقة
import os
home_dir = os.path.expanduser("~") # للحصول على المسار الصحيح لـ $HOME
cargo_bin_path = os.path.join(home_dir, ".cargo", "bin")
if cargo_bin_path not in os.environ['PATH']:
    os.environ['PATH'] = f"{cargo_bin_path}:{os.environ['PATH']}"

print("PATH updated.")
!echo $PATH # للتحقق

!which cargo

!cargo --version

!rustc --version
!cargo --version







# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/yarenty/kowalski.git
# %cd kowalski

!cargo build --release

!cargo fix --bin "kowalski"

!cargo run --release

curl -fsSL https://ollama.com/install.sh | sh
nohup ollama serve &
ollama pull llama3:8b
ollama list

!curl -fsSL https://ollama.com/install.sh | sh

!nohup ollama serve &

!ollama run llama2

عدل مسار الكتاب
/content/kowalski/src/main.rs
عدل اسم النموذج كما هو ف اولاما

!nohup ollama serve &
!ollama list

!nohup ollama serve &
!cargo run --release

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

/// Main: The entry point of our AI-powered circus.
/// "Main functions are like orchestras - they make everything work together, but nobody notices until something goes wrong."
///
/// This is where the magic happens, or at least where we pretend it does.
/// Think of it as the conductor of our AI symphony, but with more error handling.
mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader; // لا يزال موجودًا لكن لن نستخدمه في هذا المثال مع AcademicAgent
use env_logger;
use log::info;

/// Reads input from a file, because apparently typing is too mainstream.
/// "File reading is like opening presents - you never know what you're gonna get."
///
/// # Arguments
/// * `file_path` - The path to the file (which is probably too long and boring)
///
/// # Returns
/// * `Result<String, Box<dyn std::error::Error>>` - Either the file contents or an error that will make you question your career choices
#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

/// The main function that makes everything work (or at least tries to).
/// "Main functions are like first dates - they're exciting but usually end in disappointment."
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    env_logger::init();

    // Load configuration
    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    // Initialize model manager
    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    // let model_name = "michaelneale/deepseek-r1-goose";
    let model_name = "llama2:latest"; // quickest model

    // List available models
    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    // Check if default model exists and pull it if needed
    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }


    //  TODO: this is just temporary testing code
    //  TODO: remove this once we have a proper CLI interface
    //  TODO: and create examples of how to use the agents instead!
    // Initialize agents
    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?; // config تم استهلاكه هنا، لذا نحتاج إلى config.clone() إذا استخدمنا academic_agent لاحقًا

    // Example: Process a research paper (تم التعديل لإرسال سؤال نصي)
    println!("\nSending simple question to Academic Agent...");
    let conversation_id_academic = academic_agent.start_conversation(&model_name); // تم تغيير اسم المتغير لتجنب التضارب
    println!("Academic Agent Conversation ID: {}", conversation_id_academic);

    let role_academic = Role::translator(Some(Audience::Scientist), Some(Preset::Questions)); // تم تغيير اسم المتغير

    // --- بداية التعديل ---
    let mut response_academic = academic_agent // تم تغيير اسم المتغير
        .chat_with_history(
            &conversation_id_academic,
            "What is game theory in one sentence?", // <--- سؤال بسيط كنص
            Some(role_academic),
        )
        .await?;
    // --- نهاية التعديل ---

    let mut buffer_academic = String::new(); // تم تغيير اسم المتغير
    while let Some(chunk) = response_academic.chunk().await? { // تم تغيير اسم المتغير
        match academic_agent.process_stream_response(&conversation_id_academic, &chunk).await {
            Ok(Some(content)) => {
                print!("{}", content);
                io::stdout().flush()?;
                buffer_academic.push_str(&content);
            }
            Ok(None) => {
                academic_agent.add_message(&conversation_id_academic, "assistant", &buffer_academic).await;
                println!("\n");
                break;
            }
            Err(e) => {
                eprintln!("\nError processing stream: {}", e);
                break;
            }
        }
    }

    // Example: Web search and processing
    info!("Performing web search...");
    let conversation_id_tooling = tooling_agent.start_conversation(&model_name); // تم تغيير اسم المتغير
    info!("Tooling Agent Conversation ID: {}", conversation_id_tooling);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent.add_message(&conversation_id_tooling, "search", format!("{} : {}", result.title, result.snippet).as_str()).await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}", result.snippet);
        println!();
    }

    tooling_agent.add_message(&conversation_id_tooling, "user", format!("Search for {} and summary", query).as_str()).await;


    // Process the first search result
    if let Some(first_result) = search_results.first() {
        info!("\nProcessing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;

        tooling_agent.add_message(&conversation_id_tooling, "search", format!(" Full page:{} : {}", page.title, page.content).as_str()).await;

        let role_tooling = Role::translator(Some(Audience::Family), Some(Preset::Simplify)); // تم تغيير اسم المتغير
        let mut response_tooling = tooling_agent // تم تغيير اسم المتغير
            .chat_with_history(&conversation_id_tooling, "Provide simple summary", Some(role_tooling))
            .await?;

        let mut buffer_tooling = String::new(); // تم تغيير اسم المتغير
        while let Some(chunk) = response_tooling.chunk().await? { // تم تغيير اسم المتغير
            match tooling_agent.process_stream_response(&conversation_id_tooling, &chunk).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer_tooling.push_str(&content);
                }
                Ok(None) => {
                    tooling_agent.add_message(&conversation_id_tooling, "assistant", &buffer_tooling).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\nError processing stream: {}", e);
                    break;
                }
            }
        }
    }

    Ok(())
}



!nohup ollama serve &
!ollama run mistral

[ollama]
base_url = "http://localhost:11434"
default_model = "mistral-small"

[chat]
temperature = 0.7
max_tokens = 512
stream = true

https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q2_K.gguf

"""الامر

تحسين تجميع شيفرة Rust لتحسين الأداء
لتحسين أداء تجميع شيفرة Rust، يمكنك استخدام تقنيات متعددة، مثل استخدام cargo build --release للحصول على أداء أفضل. إليك شرحًا لكيفية القيام بذلك:

1. استخدام cargo build --release

عند تشغيل cargo build --release، يقوم Rust بتجميع شيفرتك مع تمكين التحسينات. هذه التحسينات تجعل الكود المُجمّع أسرع، لكنها تستغرق وقتًا أطول للتجميع.

استخدم هذا الأمر عند إنشاء إصدار نهائي من تطبيقك، حيث سيتم تنفيذه بشكل متكرر دون إعادة التجميع.

cargo build --release
Use code with caution
2. تقنيات أخرى لتحسين الأداء:

استخدام LTO (Link-Time Optimization): تمكين LTO لتحسين الكود عبر وحدات التجميع المختلفة.
أضف lto = true إلى قسم [profile.release] في ملف Cargo.toml الخاص بمشروعك.
تفعيل التحسينات الخاصة بالمعالج:
استخدم العلامة -C target-cpu=native لتفعيل تحسينات خاصة بمعالجك.
تقليل استخدام تخصيصات الذاكرة الديناميكية:
تقليل استخدام Box, Rc, و Arc عندما يكون ذلك ممكنًا لتحسين أداء الذاكرة.
استخدام أنواع بيانات فعّالة:
اختر أنواع البيانات التي تتناسب مع احتياجاتك. على سبيل المثال، استخدم u8 بدلاً من i32 إذا كنت تعلم أن القيمة لن تكون سالبة.
استخدام مكتبات مُحسّنة:
تأكد من استخدام إصدارات مُحسّنة من المكتبات التي تستخدمها.
ملاحظات هامة:

التحسينات قد تختلف: تعتمد فعالية التحسينات على طبيعة الكود الذي تقوم بتجميعه.
الوقت اللازم للتجميع: قد تستغرق التحسينات وقتًا أطول، لكنها تحسن الأداء بشكل عام.
التجربة: جرب التقنيات المختلفة لمعرفة ما يناسب مشروعك بشكل أفضل.
أتمنى أن يكون هذا الشرح مفيدًا!

### @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@شغال
"""

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

شغال
/content/kowalski/src/main.rsص


/// Main: The entry point of our AI-powered circus.
/// "Main functions are like orchestras - they make everything work together, but nobody notices until something goes wrong."
///
/// This is where the magic happens, or at least where we pretend it does.
/// Think of it as the conductor of our AI symphony, but with more error handling.
mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader; // لا يزال موجودًا لكن لن نستخدمه في هذا المثال مع AcademicAgent
use env_logger;
use log::info;

/// Reads input from a file, because apparently typing is too mainstream.
/// "File reading is like opening presents - you never know what you're gonna get."
///
/// # Arguments
/// * `file_path` - The path to the file (which is probably too long and boring)
///
/// # Returns
/// * `Result<String, Box<dyn std::error::Error>>` - Either the file contents or an error that will make you question your career choices
#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

/// The main function that makes everything work (or at least tries to).
/// "Main functions are like first dates - they're exciting but usually end in disappointment."
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    env_logger::init();

    // Load configuration
    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    // Initialize model manager
    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    // let model_name = "michaelneale/deepseek-r1-goose";
    let model_name = "llama2:latest"; // quickest model

    // List available models
    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    // Check if default model exists and pull it if needed
    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }


    //  TODO: this is just temporary testing code
    //  TODO: remove this once we have a proper CLI interface
    //  TODO: and create examples of how to use the agents instead!
    // Initialize agents
    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?; // config تم استهلاكه هنا، لذا نحتاج إلى config.clone() إذا استخدمنا academic_agent لاحقًا

    // Example: Process a research paper (تم التعديل لإرسال سؤال نصي)
    println!("\nSending simple question to Academic Agent...");
    let conversation_id_academic = academic_agent.start_conversation(&model_name); // تم تغيير اسم المتغير لتجنب التضارب
    println!("Academic Agent Conversation ID: {}", conversation_id_academic);

    let role_academic = Role::translator(Some(Audience::Scientist), Some(Preset::Questions)); // تم تغيير اسم المتغير

    // --- بداية التعديل ---
    let mut response_academic = academic_agent // تم تغيير اسم المتغير
        .chat_with_history(
            &conversation_id_academic,
            "What is game theory in one sentence?", // <--- سؤال بسيط كنص
            Some(role_academic),
        )
        .await?;
    // --- نهاية التعديل ---

    let mut buffer_academic = String::new(); // تم تغيير اسم المتغير
    while let Some(chunk) = response_academic.chunk().await? { // تم تغيير اسم المتغير
        match academic_agent.process_stream_response(&conversation_id_academic, &chunk).await {
            Ok(Some(content)) => {
                print!("{}", content);
                io::stdout().flush()?;
                buffer_academic.push_str(&content);
            }
            Ok(None) => {
                academic_agent.add_message(&conversation_id_academic, "assistant", &buffer_academic).await;
                println!("\n");
                break;
            }
            Err(e) => {
                eprintln!("\nError processing stream: {}", e);
                break;
            }
        }
    }

    // Example: Web search and processing
    info!("Performing web search...");
    let conversation_id_tooling = tooling_agent.start_conversation(&model_name); // تم تغيير اسم المتغير
    info!("Tooling Agent Conversation ID: {}", conversation_id_tooling);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent.add_message(&conversation_id_tooling, "search", format!("{} : {}", result.title, result.snippet).as_str()).await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}", result.snippet);
        println!();
    }

    tooling_agent.add_message(&conversation_id_tooling, "user", format!("Search for {} and summary", query).as_str()).await;


    // Process the first search result
    if let Some(first_result) = search_results.first() {
        info!("\nProcessing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;

        tooling_agent.add_message(&conversation_id_tooling, "search", format!(" Full page:{} : {}", page.title, page.content).as_str()).await;

        let role_tooling = Role::translator(Some(Audience::Family), Some(Preset::Simplify)); // تم تغيير اسم المتغير
        let mut response_tooling = tooling_agent // تم تغيير اسم المتغير
            .chat_with_history(&conversation_id_tooling, "Provide simple summary", Some(role_tooling))
            .await?;

        let mut buffer_tooling = String::new(); // تم تغيير اسم المتغير
        while let Some(chunk) = response_tooling.chunk().await? { // تم تغيير اسم المتغير
            match tooling_agent.process_stream_response(&conversation_id_tooling, &chunk).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer_tooling.push_str(&content);
                }
                Ok(None) => {
                    tooling_agent.add_message(&conversation_id_tooling, "assistant", &buffer_tooling).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\nError processing stream: {}", e);
                    break;
                }
            }
        }
    }

    Ok(())
}

"""@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

ayhgشغال
"""

/// Main: The entry point of our AI-powered circus.
/// "Main functions are like orchestras - they make everything work together, but nobody notices until something goes wrong."
///
/// This is where the magic happens, or at least where we pretend it does.
/// Think of it as the conductor of our AI symphony, but with more error handling.
mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader; // لا يزال موجودًا لكن لن نستخدمه في هذا المثال مع AcademicAgent
use env_logger;
use log::info;

/// Reads input from a file, because apparently typing is too mainstream.
/// "File reading is like opening presents - you never know what you're gonna get."
///
/// # Arguments
/// * `file_path` - The path to the file (which is probably too long and boring)
///
/// # Returns
/// * `Result<String, Box<dyn std::error::Error>>` - Either the file contents or an error that will make you question your career choices
#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

/// The main function that makes everything work (or at least tries to).
/// "Main functions are like first dates - they're exciting but usually end in disappointment."
#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // Initialize logging
    env_logger::init();

    // Load configuration
    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    // Initialize model manager
    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    // let model_name = "michaelneale/deepseek-r1-goose";
    let model_name = "llama2:latest"; // quickest model

    // List available models
    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    // Check if default model exists and pull it if needed
    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }


    //  TODO: this is just temporary testing code
    //  TODO: remove this once we have a proper CLI interface
    //  TODO: and create examples of how to use the agents instead!
    // Initialize agents
    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?; // config تم استهلاكه هنا، لذا نحتاج إلى config.clone() إذا استخدمنا academic_agent لاحقًا

    // Example: Process a research paper (تم التعديل لإرسال سؤال نصي)
    println!("\nSending simple question to Academic Agent...");
    let conversation_id_academic = academic_agent.start_conversation(&model_name); // تم تغيير اسم المتغير لتجنب التضارب
    println!("Academic Agent Conversation ID: {}", conversation_id_academic);

    let role_academic = Role::translator(Some(Audience::Scientist), Some(Preset::Questions)); // تم تغيير اسم المتغير

    // --- بداية التعديل ---
    let mut response_academic = academic_agent // تم تغيير اسم المتغير
        .chat_with_history(
            &conversation_id_academic,
            "What is game theory in one sentence?", // <--- سؤال بسيط كنص
            Some(role_academic),
        )
        .await?;
    // --- نهاية التعديل ---

    let mut buffer_academic = String::new(); // تم تغيير اسم المتغير
    while let Some(chunk) = response_academic.chunk().await? { // تم تغيير اسم المتغير
        match academic_agent.process_stream_response(&conversation_id_academic, &chunk).await {
            Ok(Some(content)) => {
                print!("{}", content);
                io::stdout().flush()?;
                buffer_academic.push_str(&content);
            }
            Ok(None) => {
                academic_agent.add_message(&conversation_id_academic, "assistant", &buffer_academic).await;
                println!("\n");
                break;
            }
            Err(e) => {
                eprintln!("\nError processing stream: {}", e);
                break;
            }
        }
    }

    // Example: Web search and processing
    info!("Performing web search...");
    let conversation_id_tooling = tooling_agent.start_conversation(&model_name); // تم تغيير اسم المتغير
    info!("Tooling Agent Conversation ID: {}", conversation_id_tooling);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent.add_message(&conversation_id_tooling, "search", format!("{} : {}", result.title, result.snippet).as_str()).await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}", result.snippet);
        println!();
    }

    tooling_agent.add_message(&conversation_id_tooling, "user", format!("Search for {} and summary", query).as_str()).await;


    // Process the first search result
    if let Some(first_result) = search_results.first() {
        info!("\nProcessing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;

        tooling_agent.add_message(&conversation_id_tooling, "search", format!(" Full page:{} : {}", page.title, page.content).as_str()).await;

        let role_tooling = Role::translator(Some(Audience::Family), Some(Preset::Simplify)); // تم تغيير اسم المتغير
        let mut response_tooling = tooling_agent // تم تغيير اسم المتغير
            .chat_with_history(&conversation_id_tooling, "Provide simple summary", Some(role_tooling))
            .await?;

        let mut buffer_tooling = String::new(); // تم تغيير اسم المتغير
        while let Some(chunk) = response_tooling.chunk().await? { // تم تغيير اسم المتغير
            match tooling_agent.process_stream_response(&conversation_id_tooling, &chunk).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer_tooling.push_str(&content);
                }
                Ok(None) => {
                    tooling_agent.add_message(&conversation_id_tooling, "assistant", &buffer_tooling).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\nError processing stream: {}", e);
                    break;
                }
            }
        }
    }

    Ok(())
}











mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader;
use env_logger;
use log::info;
use tokio::time::{timeout, Duration};

#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();
    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    let model_name = "llama2:latest"; // نموذج سريع وخفيف

    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }

    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?;

    println!("\n📄 Processing research paper...");
    let conversation_id = academic_agent.start_conversation(&model_name);
    println!("Academic Agent Conversation ID: {}", conversation_id);

    let role = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
    let mut response = academic_agent
        .chat_with_history(&conversation_id, "/content/game-theory.pdf", Some(role))
        .await?;

    let mut buffer = String::new();
    let mut total_chars = 0;

    println!("🧠 Thinking...");
    while let Ok(Ok(chunk)) = timeout(Duration::from_secs(10), response.chunk()).await {
        match academic_agent.process_stream_response(&conversation_id, &chunk).await {
            Ok(Some(content)) => {
                print!("{}", content);
                io::stdout().flush()?;
                buffer.push_str(&content);
                total_chars += content.len();
                if total_chars > 1000 {
                    println!("\n✂️ Output truncated after 1000 characters.");
                    break;
                }
            }
            Ok(None) => {
                academic_agent.add_message(&conversation_id, "assistant", &buffer).await;
                println!("\n");
                break;
            }
            Err(e) => {
                eprintln!("\n⚠️ Error processing stream: {}", e);
                break;
            }
        }
    }

    info!("🌐 Performing web search...");
    let conversation_id = tooling_agent.start_conversation(&model_name);
    info!("Tooling Agent Conversation ID: {}", conversation_id);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent
            .add_message(&conversation_id, "search", format!("{} : {}", result.title, result.snippet).as_str())
            .await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}\n", result.snippet);
    }

    tooling_agent
        .add_message(&conversation_id, "user", format!("Search for {} and summary", query).as_str())
        .await;

    if let Some(first_result) = search_results.first() {
        info!("📄 Fetching and summarizing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;
        tooling_agent
            .add_message(&conversation_id, "search", format!(" Full page:{} : {}", page.title, page.content).as_str())
            .await;

        let role = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
        let mut response = tooling_agent
            .chat_with_history(&conversation_id, "Provide simple summary", Some(role))
            .await?;

        let mut buffer = String::new();
        let mut total_chars = 0;
        println!("💬 Generating summary...");

        while let Ok(Ok(chunk)) = timeout(Duration::from_secs(10), response.chunk()).await {
            match tooling_agent.process_stream_response(&conversation_id, &chunk).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer.push_str(&content);
                    total_chars += content.len();
                    if total_chars > 1000 {
                        println!("\n✂️ Output truncated after 1000 characters.");
                        break;
                    }
                }
                Ok(None) => {
                    tooling_agent.add_message(&conversation_id, "assistant", &buffer).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\n⚠️ Error processing stream: {}", e);
                    break;
                }
            }
        }
    }

    Ok(())
}

mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader;
use env_logger;
use log::info;

#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();

    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    let model_name = "llama2:latest";

    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }

    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?;

    println!("\nProcessing research paper...");
    let conversation_id = academic_agent.start_conversation(&model_name);
    println!("Academic Agent Conversation ID: {}", conversation_id);

    let role = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
    let mut response = academic_agent
        .chat_with_history(
            &conversation_id,
            "/content/game-theory.pdf",
            Some(role),
        )
        .await?;

    let mut buffer = String::new();
    while let Some(chunk) = response.chunk().await? {
        if let Some(bytes) = &chunk {
            match academic_agent.process_stream_response(&conversation_id, bytes).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer.push_str(&content);
                }
                Ok(None) => {
                    academic_agent.add_message(&conversation_id, "assistant", &buffer).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\nError processing stream: {}", e);
                    break;
                }
            }
        }
    }

    info!("Performing web search...");
    let conversation_id = tooling_agent.start_conversation(&model_name);
    info!("Tooling Agent Conversation ID: {}", conversation_id);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent.add_message(
            &conversation_id,
            "search",
            format!("{} : {}", result.title, result.snippet).as_str(),
        ).await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}", result.snippet);
        println!();
    }

    tooling_agent.add_message(
        &conversation_id,
        "user",
        format!("Search for {} and summary", query).as_str(),
    ).await;

    if let Some(first_result) = search_results.first() {
        info!("\nProcessing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;

        tooling_agent.add_message(
            &conversation_id,
            "search",
            format!(" Full page:{} : {}", page.title, page.content).as_str(),
        ).await;

        let role = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
        let mut response = tooling_agent
            .chat_with_history(&conversation_id, "Provide simple summary", Some(role))
            .await?;

        let mut buffer = String::new();
        while let Some(chunk) = response.chunk().await? {
            if let Some(bytes) = &chunk {
                match tooling_agent.process_stream_response(&conversation_id, bytes).await {
                    Ok(Some(content)) => {
                        print!("{}", content);
                        io::stdout().flush()?;
                        buffer.push_str(&content);
                    }
                    Ok(None) => {
                        tooling_agent.add_message(&conversation_id, "assistant", &buffer).await;
                        println!("\n");
                        break;
                    }
                    Err(e) => {
                        eprintln!("\nError processing stream: {}", e);
                        break;
                    }
                }
            }
        }
    }

    Ok(())
}

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

What is game theory in one sentence?

mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

use agent::{Agent, AcademicAgent, ToolingAgent};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::fs;
use std::io::{self, Write};
use utils::PdfReader;
use env_logger;
use log::info;

#[allow(dead_code)]
fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
    if file_path.to_lowercase().ends_with(".pdf") {
        Ok(PdfReader::read_pdf_file(file_path)?)
    } else {
        Ok(fs::read_to_string(file_path)?)
    }
}

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    env_logger::init();

    let config = config::Config::load()?;
    info!("Loaded configuration with search provider: {}", config.search.provider);

    let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
    let model_name = "llama2:latest";

    info!("Listing available models...");
    let models = model_manager.list_models().await?;
    for model in models.models {
        println!(
            "Model: {}, Size: {} bytes, Modified: {}",
            model.name, model.size, model.modified_at
        );
    }

    if !model_manager.model_exists(&model_name).await? {
        println!("Pulling model {}...", model_name);
        let mut stream = model_manager.pull_model(&model_name).await?;
        while let Some(chunk) = stream.chunk().await? {
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    print!("Status: {}\r", status);
                    io::stdout().flush()?;
                }
            }
        }
        println!("\nModel pulled successfully!");
    }

    let mut academic_agent = AcademicAgent::new(config.clone())?;
    let mut tooling_agent = ToolingAgent::new(config)?;

    println!("\nProcessing research paper...");
    let conversation_id = academic_agent.start_conversation(&model_name);
    println!("Academic Agent Conversation ID: {}", conversation_id);

    let role = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
    let mut response = academic_agent
        .chat_with_history(
            &conversation_id,
            "/content/game-theory.pdf",
            Some(role),
        )
        .await?;

    let mut buffer = String::new();
    while let Some(chunk) = response.chunk().await? {
        if let Some(bytes) = &chunk {
            match academic_agent.process_stream_response(&conversation_id, bytes).await {
                Ok(Some(content)) => {
                    print!("{}", content);
                    io::stdout().flush()?;
                    buffer.push_str(&content);
                }
                Ok(None) => {
                    academic_agent.add_message(&conversation_id, "assistant", &buffer).await;
                    println!("\n");
                    break;
                }
                Err(e) => {
                    eprintln!("\nError processing stream: {}", e);
                    break;
                }
            }
        }
    }

    info!("Performing web search...");
    let conversation_id = tooling_agent.start_conversation(&model_name);
    info!("Tooling Agent Conversation ID: {}", conversation_id);

    let query = "Latest developments in Rust programming";
    let search_results = tooling_agent.search(query).await?;

    for result in &search_results {
        tooling_agent.add_message(
            &conversation_id,
            "search",
            format!("{} : {}", result.title, result.snippet).as_str(),
        ).await;
        println!("Title: {}", result.title);
        println!("URL: {}", result.url);
        println!("Snippet: {}", result.snippet);
        println!();
    }

    tooling_agent.add_message(
        &conversation_id,
        "user",
        format!("Search for {} and summary", query).as_str(),
    ).await;

    if let Some(first_result) = search_results.first() {
        info!("\nProcessing first search result...");
        let page = tooling_agent.fetch_page(&first_result.url).await?;

        tooling_agent.add_message(
            &conversation_id,
            "search",
            format!(" Full page:{} : {}", page.title, page.content).as_str(),
        ).await;

        let role = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
        let mut response = tooling_agent
            .chat_with_history(&conversation_id, "Provide simple summary", Some(role))
            .await?;

        let mut buffer = String::new();
        while let Some(chunk) = response.chunk().await? {
            if let Some(bytes) = &chunk {
                match tooling_agent.process_stream_response(&conversation_id, bytes).await {
                    Ok(Some(content)) => {
                        print!("{}", content);
                        io::stdout().flush()?;
                        buffer.push_str(&content);
                    }
                    Ok(None) => {
                        tooling_agent.add_message(&conversation_id, "assistant", &buffer).await;
                        println!("\n");
                        break;
                    }
                    Err(e) => {
                        eprintln!("\nError processing stream: {}", e);
                        break;
                    }
                }
            }
        }
    }

    Ok(())
}

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/main.rs
# /// Main: The entry point of our AI-powered circus.
# /// "Main functions are like orchestras - they make everything work together, but nobody notices until something goes wrong."
# ///
# /// This is where the magic happens, or at least where we pretend it does.
# /// Think of it as the conductor of our AI symphony, but with more error handling.
# mod agent;
# mod config;
# mod conversation;
# mod model;
# mod role;
# mod utils;
# mod tools;
# 
# use agent::{Agent, AcademicAgent, ToolingAgent};
# use model::ModelManager;
# use role::{Audience, Preset, Role};
# use serde_json::Value;
# use std::fs;
# use std::io::{self, Write}; // io::Write needed for flush
# use utils::PdfReader;
# use env_logger;
# use log::info;
# 
# /// Reads input from a file, because apparently typing is too mainstream.
# /// "File reading is like opening presents - you never know what you're gonna get."
# ///
# /// # Arguments
# /// * `file_path` - The path to the file (which is probably too long and boring)
# ///
# /// # Returns
# /// * `Result<String, Box<dyn std::error::Error>>` - Either the file contents or an error that will make you question your career choices
# #[allow(dead_code)]
# fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
#     if file_path.to_lowercase().ends_with(".pdf") {
#         Ok(PdfReader::read_pdf_file(file_path)?)
#     } else {
#         Ok(fs::read_to_string(file_path)?)
#     }
# }
# 
# /// The main function that makes everything work (or at least tries to).
# /// "Main functions are like first dates - they're exciting but usually end in disappointment."
# #[tokio::main] // This attribute macro makes the async main function work with Tokio
# async fn main() -> Result<(), Box<dyn std::error::Error>> {
#     // Initialize logging
#     env_logger::init();
# 
#     // Load configuration
#     let config = config::Config::load()?;
#     info!("Loaded configuration with search provider: {}", config.search.provider);
# 
#     // Initialize model manager
#     let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
#     // let model_name = "michaelneale/deepseek-r1-goose"; // Example of another model
#     let model_name = "llama2:latest"; // Using "llama2" (will resolve to llama2:latest by default in Ollama)
# 
#     // List available models
#     info!("Listing available models...");
#     let models = model_manager.list_models().await?;
#     for model in models.models {
#         println!(
#             "Model: {}, Size: {} bytes, Modified: {}",
#             model.name, model.size, model.modified_at
#         );
#     }
# 
#     // Check if default model exists and pull it if needed
#     if !model_manager.model_exists(&model_name).await? {
#         println!("Pulling model {} (this might be {} or {} if not fully qualified)...", model_name, model_name, format!("{}:latest", model_name));
#         let mut stream = model_manager.pull_model(&model_name).await?; // Ollama handles "llama2" as "llama2:latest"
#         while let Some(chunk) = stream.chunk().await? {
#             if let Ok(text) = String::from_utf8(chunk.to_vec()) {
#                 let v: Value = serde_json::from_str(&text)?;
#                 if let Some(status) = v["status"].as_str() {
#                     print!("Status: {}\r", status);
#                     io::stdout().flush()?;
#                 }
#             }
#         }
#         println!("\nModel pulled successfully!");
#     } else {
#         info!("Model {} already exists.", model_name);
#     }
# 
#     // Initialize agents
#     let mut academic_agent = AcademicAgent::new(config.clone())?;
#     let mut tooling_agent = ToolingAgent::new(config)?;
# 
#     // --- Academic Agent Interaction ---
#     println!("\n--- Academic Agent ---");
#     let conversation_id_academic = academic_agent.start_conversation(&model_name);
#     println!("Academic Agent Conversation ID: {}", conversation_id_academic);
# 
#     println!("Do you want to provide a PDF file path or type a question directly?");
#     println!("1. PDF file path");
#     println!("2. Type a question");
#     print!("Enter your choice (1 or 2): ");
#     io::stdout().flush()?;
# 
#     let mut choice = String::new();
#     io::stdin().read_line(&mut choice)?;
#     let choice = choice.trim();
# 
#     let input_content_academic: String; // Renamed to avoid conflict
# 
#     if choice == "1" {
#         print!("Enter the path to your PDF file: ");
#         io::stdout().flush()?;
#         let mut file_path_input = String::new();
#         io::stdin().read_line(&mut file_path_input)?;
#         let file_path = file_path_input.trim();
# 
#         if !file_path.is_empty() {
#             info!("Reading PDF file: {}", file_path);
#             match read_input_file(file_path) {
#                 Ok(pdf_text) => {
#                     print!("Enter your question about the PDF: ");
#                     io::stdout().flush()?;
#                     let mut pdf_question = String::new();
#                     io::stdin().read_line(&mut pdf_question)?;
#                     // For simplicity, sending only a part of PDF and the question.
#                     // Real RAG would be more complex.
#                     input_content_academic = format!(
#                         "Based on the following document content (first 2000 chars):\n---\n{}...\n---\nAnswer this question: {}",
#                         pdf_text.chars().take(2000).collect::<String>(),
#                         pdf_question.trim()
#                     );
#                     info!("Processing PDF content and your question...");
#                 }
#                 Err(e) => {
#                     eprintln!("Error reading PDF file: {}. Using a default question.", e);
#                     input_content_academic = "What is game theory in one sentence?".to_string();
#                 }
#             }
#         } else {
#             println!("No PDF path provided. Using a default question.");
#             input_content_academic = "What is game theory in one sentence?".to_string();
#         }
#     } else if choice == "2" {
#         print!("Enter your question for the Academic Agent: ");
#         io::stdout().flush()?;
#         let mut user_question = String::new();
#         io::stdin().read_line(&mut user_question)?;
#         input_content_academic = user_question.trim().to_string();
#         if input_content_academic.is_empty() {
#             println!("No question entered. Using a default question.");
#             input_content_academic = "What is game theory in one sentence?".to_string();
#         }
#     } else {
#         println!("Invalid choice. Using a default question.");
#         input_content_academic = "What is game theory in one sentence?".to_string();
#     }
# 
#     let role_academic = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
# 
#     let mut response_academic = academic_agent
#         .chat_with_history(
#             &conversation_id_academic,
#             &input_content_academic,
#             Some(role_academic),
#         )
#         .await?;
# 
#     let mut buffer_academic = String::new();
#     println!("\nAcademic Agent Response:");
#     while let Some(chunk) = response_academic.chunk().await? {
#         match academic_agent.process_stream_response(&conversation_id_academic, &chunk).await {
#             Ok(Some(content)) => {
#                 print!("{}", content);
#                 io::stdout().flush()?;
#                 buffer_academic.push_str(&content);
#             }
#             Ok(None) => {
#                 academic_agent.add_message(&conversation_id_academic, "assistant", &buffer_academic).await;
#                 println!("\n--- End of Academic Agent Response ---");
#                 break;
#             }
#             Err(e) => {
#                 eprintln!("\nError processing stream for Academic Agent: {}", e);
#                 break;
#             }
#         }
#     }
# 
#     // --- Tooling Agent Interaction ---
#     info!("\n--- Tooling Agent ---");
#     let conversation_id_tooling = tooling_agent.start_conversation(&model_name);
#     info!("Tooling Agent Conversation ID: {}", conversation_id_tooling);
# 
#     print!("Enter your search query for the Tooling Agent (e.g., Latest developments in Rust programming): ");
#     io::stdout().flush()?;
#     let mut user_search_query = String::new();
#     io::stdin().read_line(&mut user_search_query)?;
#     let query_tooling = user_search_query.trim(); // Renamed to avoid conflict
# 
#     if query_tooling.is_empty() {
#         println!("No search query entered. Skipping web search.");
#     } else {
#         info!("Performing web search for: {}", query_tooling);
#         let search_results = tooling_agent.search(query_tooling).await?;
# 
#         for result in &search_results {
#             tooling_agent.add_message(&conversation_id_tooling, "search", format!("{} : {}", result.title, result.snippet).as_str()).await;
#             println!("Title: {}", result.title);
#             println!("URL: {}", result.url);
#             // println!("Snippet: {}", result.snippet); // Can be verbose
#             println!();
#         }
# 
#         tooling_agent.add_message(&conversation_id_tooling, "user", format!("Search for {} and summarize the first result.", query_tooling).as_str()).await;
# 
#         if let Some(first_result) = search_results.first() {
#             info!("\nProcessing first search result: {}", first_result.url);
#             match tooling_agent.fetch_page(&first_result.url).await {
#                 Ok(page) => {
#                     tooling_agent.add_message(&conversation_id_tooling, "search", format!("Full page content from {}: {}...", page.url, page.content.chars().take(500).collect::<String>()).as_str()).await;
# 
#                     let role_tooling = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
#                     let prompt_summary = "Provide a simple summary of the fetched page content.";
#                     let mut response_tooling = tooling_agent
#                         .chat_with_history(&conversation_id_tooling, prompt_summary, Some(role_tooling))
#                         .await?;
# 
#                     let mut buffer_tooling = String::new();
#                     println!("\nTooling Agent Summary Response:");
#                     while let Some(chunk) = response_tooling.chunk().await? {
#                         match tooling_agent.process_stream_response(&conversation_id_tooling, &chunk).await {
#                             Ok(Some(content)) => {
#                                 print!("{}", content);
#                                 io::stdout().flush()?;
#                                 buffer_tooling.push_str(&content);
#                             }
#                             Ok(None) => {
#                                 tooling_agent.add_message(&conversation_id_tooling, "assistant", &buffer_tooling).await;
#                                 println!("\n--- End of Tooling Agent Summary ---");
#                                 break;
#                             }
#                             Err(e) => {
#                                 eprintln!("\nError processing stream for Tooling Agent: {}", e);
#                                 break;
#                             }
#                         }
#                     }
#                 }
#                 Err(e) => {
#                     eprintln!("Failed to fetch page {}: {}", first_result.url, e);
#                 }
#             }
#         } else {
#             println!("No search results to process.");
#         }
#     }
# 
#     println!("\nApplication finished.");
#     Ok(())
# }

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

"""### شغال جيد ولكن يغيب%%%%%%%%%%%%%%%%%%%%%"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile src/main.rs
# /// Main: The entry point of our AI-powered circus.
# // ... (التعليقات كما هي)
# mod agent;
# mod config;
# mod conversation;
# mod model;
# mod role;
# mod utils;
# mod tools;
# 
# use agent::{Agent, AcademicAgent, ToolingAgent};
# use model::ModelManager;
# use role::{Audience, Preset, Role};
# use serde_json::Value;
# use std::fs;
# use std::io::{self, Write}; // io::Write needed for flush
# use utils::PdfReader;
# use env_logger;
# use log::info;
# 
# /// Reads input from a file, because apparently typing is too mainstream.
# // ... (بقية الدالة كما هي)
# #[allow(dead_code)]
# fn read_input_file(file_path: &str) -> Result<String, Box<dyn std::error::Error>> {
#     if file_path.to_lowercase().ends_with(".pdf") {
#         Ok(PdfReader::read_pdf_file(file_path)?)
#     } else {
#         Ok(fs::read_to_string(file_path)?)
#     }
# }
# 
# /// The main function that makes everything work (or at least tries to).
# // ... (بقية التعليقات كما هي)
# #[tokio::main] // This attribute macro makes the async main function work with Tokio
# async fn main() -> Result<(), Box<dyn std::error::Error>> {
#     // Initialize logging
#     env_logger::init();
# 
#     // Load configuration
#     let config = config::Config::load()?;
#     info!("Loaded configuration with search provider: {}", config.search.provider);
# 
#     // Initialize model manager
#     let model_manager = ModelManager::new(config.ollama.base_url.clone())?;
#     let model_name = "llama2";
# 
#     // List available models
#     info!("Listing available models...");
#     let models = model_manager.list_models().await?;
#     for model in models.models {
#         println!(
#             "Model: {}, Size: {} bytes, Modified: {}",
#             model.name, model.size, model.modified_at
#         );
#     }
# 
#     // Check if default model exists and pull it if needed
#     if !model_manager.model_exists(&model_name).await? {
#         println!("Pulling model {} (this might be {} or {} if not fully qualified)...", model_name, model_name, format!("{}:latest", model_name));
#         let mut stream = model_manager.pull_model(&model_name).await?;
#         while let Some(chunk) = stream.chunk().await? {
#             if let Ok(text) = String::from_utf8(chunk.to_vec()) {
#                 let v: Value = serde_json::from_str(&text)?;
#                 if let Some(status) = v["status"].as_str() {
#                     print!("Status: {}\r", status);
#                     io::stdout().flush()?;
#                 }
#             }
#         }
#         println!("\nModel pulled successfully!");
#     } else {
#         info!("Model {} already exists.", model_name);
#     }
# 
#     // Initialize agents
#     let mut academic_agent = AcademicAgent::new(config.clone())?;
#     let mut tooling_agent = ToolingAgent::new(config)?;
# 
#     // --- Academic Agent Interaction ---
#     println!("\n--- Academic Agent ---");
#     let conversation_id_academic = academic_agent.start_conversation(&model_name);
#     println!("Academic Agent Conversation ID: {}", conversation_id_academic);
# 
#     println!("Do you want to provide a PDF file path or type a question directly?");
#     println!("1. PDF file path");
#     println!("2. Type a question");
#     print!("Enter your choice (1 or 2): ");
#     io::stdout().flush()?;
# 
#     let mut choice = String::new();
#     io::stdin().read_line(&mut choice)?;
#     let choice = choice.trim();
# 
#     // *** التصحيح هنا ***
#     let mut input_content_academic: String; // جعل المتغير قابلاً للتغيير
# 
#     if choice == "1" {
#         print!("Enter the path to your PDF file: ");
#         io::stdout().flush()?;
#         let mut file_path_input = String::new();
#         io::stdin().read_line(&mut file_path_input)?;
#         let file_path = file_path_input.trim();
# 
#         if !file_path.is_empty() {
#             info!("Reading PDF file: {}", file_path);
#             match read_input_file(file_path) {
#                 Ok(pdf_text) => {
#                     print!("Enter your question about the PDF: ");
#                     io::stdout().flush()?;
#                     let mut pdf_question = String::new();
#                     io::stdin().read_line(&mut pdf_question)?;
#                     input_content_academic = format!(
#                         "Based on the following document content (first 2000 chars):\n---\n{}...\n---\nAnswer this question: {}",
#                         pdf_text.chars().take(2000).collect::<String>(),
#                         pdf_question.trim()
#                     );
#                     info!("Processing PDF content and your question...");
#                 }
#                 Err(e) => {
#                     eprintln!("Error reading PDF file: {}. Using a default question.", e);
#                     input_content_academic = "What is game theory in one sentence?".to_string();
#                 }
#             }
#         } else {
#             println!("No PDF path provided. Using a default question.");
#             input_content_academic = "What is game theory in one sentence?".to_string();
#         }
#     } else if choice == "2" {
#         print!("Enter your question for the Academic Agent: ");
#         io::stdout().flush()?;
#         let mut user_question = String::new();
#         io::stdin().read_line(&mut user_question)?;
#         input_content_academic = user_question.trim().to_string(); // التعيين الأول
#         if input_content_academic.is_empty() {
#             println!("No question entered. Using a default question.");
#             input_content_academic = "What is game theory in one sentence?".to_string(); // التعيين الثاني (الآن مسموح به)
#         }
#     } else {
#         println!("Invalid choice. Using a default question.");
#         input_content_academic = "What is game theory in one sentence?".to_string();
#     }
# 
#     let role_academic = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
# 
#     let mut response_academic = academic_agent
#         .chat_with_history(
#             &conversation_id_academic,
#             &input_content_academic,
#             Some(role_academic),
#         )
#         .await?;
# 
#     let mut buffer_academic = String::new();
#     println!("\nAcademic Agent Response:");
#     while let Some(chunk) = response_academic.chunk().await? {
#         match academic_agent.process_stream_response(&conversation_id_academic, &chunk).await {
#             Ok(Some(content)) => {
#                 print!("{}", content);
#                 io::stdout().flush()?;
#                 buffer_academic.push_str(&content);
#             }
#             Ok(None) => {
#                 academic_agent.add_message(&conversation_id_academic, "assistant", &buffer_academic).await;
#                 println!("\n--- End of Academic Agent Response ---");
#                 break;
#             }
#             Err(e) => {
#                 eprintln!("\nError processing stream for Academic Agent: {}", e);
#                 break;
#             }
#         }
#     }
# 
#     // --- Tooling Agent Interaction ---
#     info!("\n--- Tooling Agent ---");
#     let conversation_id_tooling = tooling_agent.start_conversation(&model_name);
#     info!("Tooling Agent Conversation ID: {}", conversation_id_tooling);
# 
#     print!("Enter your search query for the Tooling Agent (e.g., Latest developments in Rust programming): ");
#     io::stdout().flush()?;
#     let mut user_search_query = String::new();
#     io::stdin().read_line(&mut user_search_query)?;
#     let query_tooling = user_search_query.trim();
# 
#     if query_tooling.is_empty() {
#         println!("No search query entered. Skipping web search.");
#     } else {
#         info!("Performing web search for: {}", query_tooling);
#         let search_results = tooling_agent.search(query_tooling).await?;
# 
#         for result in &search_results {
#             tooling_agent.add_message(&conversation_id_tooling, "search", format!("{} : {}", result.title, result.snippet).as_str()).await;
#             println!("Title: {}", result.title);
#             println!("URL: {}", result.url);
#             println!();
#         }
# 
#         tooling_agent.add_message(&conversation_id_tooling, "user", format!("Search for {} and summarize the first result.", query_tooling).as_str()).await;
# 
#         if let Some(first_result) = search_results.first() {
#             info!("\nProcessing first search result: {}", first_result.url);
#             match tooling_agent.fetch_page(&first_result.url).await {
#                 Ok(page) => {
#                     tooling_agent.add_message(&conversation_id_tooling, "search", format!("Full page content from {}: {}...", page.url, page.content.chars().take(500).collect::<String>()).as_str()).await;
# 
#                     let role_tooling = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
#                     let prompt_summary = "Provide a simple summary of the fetched page content.";
#                     let mut response_tooling = tooling_agent
#                         .chat_with_history(&conversation_id_tooling, prompt_summary, Some(role_tooling))
#                         .await?;
# 
#                     let mut buffer_tooling = String::new();
#                     println!("\nTooling Agent Summary Response:");
#                     while let Some(chunk) = response_tooling.chunk().await? {
#                         match tooling_agent.process_stream_response(&conversation_id_tooling, &chunk).await {
#                             Ok(Some(content)) => {
#                                 print!("{}", content);
#                                 io::stdout().flush()?;
#                                 buffer_tooling.push_str(&content);
#                             }
#                             Ok(None) => {
#                                 tooling_agent.add_message(&conversation_id_tooling, "assistant", &buffer_tooling).await;
#                                 println!("\n--- End of Tooling Agent Summary ---");
#                                 break;
#                             }
#                             Err(e) => {
#                                 eprintln!("\nError processing stream for Tooling Agent: {}", e);
#                                 break;
#                             }
#                         }
#                     }
#                 }
#                 Err(e) => {
#                     eprintln!("Failed to fetch page {}: {}", first_result.url, e);
#                 }
#             }
#         } else {
#             println!("No search results to process.");
#         }
#     }
# 
#     println!("\nApplication finished.");
#     Ok(())
# }

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/kowalski
!nohup ollama serve &
!cargo run --release

"""### %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%"""

















"""الحل الأمثل على المدى الطويل للتعامل مع ملفات PDF هو تنفيذ استراتيجية RAG (Retrieval Augmented Generation) كاملة:
قسّم PDF إلى أجزاء صغيرة.
قم بإنشاء تضمينات (embeddings) لهذه الأجزاء.
عندما يسأل المستخدم، ابحث عن الأجزاء الأكثر صلة فقط.
أرسل هذه الأجزاء ذات الصلة (سياق أصغر بكثير) إلى النموذج اللغوي.
مشروع
"""









https://github.com/yarenty/kowalski/issues/1

// Example CLI structure
   kowalski chat "What's the meaning of life?"
   kowalski pdf analyze research-paper.pdf
   kowalski model list

!kowalski chat "What's the meaning of life?"

تحسين الكود الاساسى من الريبو
/// Main: نظام ذكاء اصطناعي مع تحسينات الأداء والاستجابة
/// تمت إضافة واجهة مستخدم تفاعلية، معالجة أخطاء موسعة، وتحسين إدارة الموارد
use agent::{AcademicAgent, ToolingAgent};
use config::Config;
use crossterm::{
    event::{self, Event, KeyCode},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::{
    error::Error,
    fs,
    io::{self, stdout, Write},
    time::Duration,
};
use tui::{backend::CrosstermBackend, widgets::{Widget, Block, Borders}, Terminal};
use utils::PdfReader;

mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

// إعدادات النظام الأساسية
const TIMEOUT_DURATION: u64 = 30; // ثانية
const MAX_FILE_SIZE: usize = 10_000_000; // 10MB

/// نقطة الدخول الرئيسية مع معالجة أخطاء موسعة
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    // تهيئة التسجيل
    env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();

    // تهيئة الواجهة الطرفية
    let mut stdout = stdout();
    execute!(stdout, EnterAlternateScreen)?;
    enable_raw_mode()?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    // تحميل الإعدادات
    let config = Config::load().map_err(|e| format!("فشل تحميل الإعدادات: {}", e))?;

    // إدارة النماذج
    let model_manager = ModelManager::new(config.ollama.base_url.clone())
        .map_err(|e| format!("فشل تهيئة مدير النماذج: {}", e))?;

    // التحقق من النموذج الأساسي
    let model_name = "llama2";
    handle_model_setup(&model_manager, model_name).await?;

    // تهيئة الوكلاء
    let academic_agent = AcademicAgent::new(config.clone())
        .map_err(|e| format!("فشل تهيئة الوكيل الأكاديمي: {}", e))?;
    let tooling_agent = ToolingAgent::new(config)
        .map_err(|e| format!("فشل تهيئة الوكيل الأدوات: {}", e))?;

    // الواجهة التفاعلية الرئيسية
    run_interactive_ui(&model_manager, academic_agent, tooling_agent, &mut terminal).await?;

    // تنظيف الواجهة الطرفية
    disable_raw_mode()?;
    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
    Ok(())
}

/// إعداد النموذج مع التحقق والتخزين المؤقت
async fn handle_model_setup(
    model_manager: &ModelManager,
    model_name: &str,
) -> Result<(), Box<dyn Error>> {
    if !model_manager.model_exists(model_name).await? {
        log::info!("جاري تنزيل النموذج {}...", model_name);
        let mut stream = model_manager
            .pull_model(model_name)
            .await
            .map_err(|e| format!("فشل في سحب النموذج: {}", e))?;

        let start_time = tokio::time::Instant::now();
        while let Some(chunk) = tokio::time::timeout(
            Duration::from_secs(TIMEOUT_DURATION),
            stream.chunk(),
        )
        .await?
        {
            let chunk = chunk?;
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    log::debug!("حالة التنزيل: {}", status);
                }
            }
        }

        model_manager.cache_model(model_name).await?;
        log::info!("تم التخزين المؤقت للنموذج في {} ثانية", start_time.elapsed().as_secs());
    }
    Ok(())
}

/// الواجهة التفاعلية الرئيسية
async fn run_interactive_ui(
    model_manager: &ModelManager,
    academic_agent: AcademicAgent,
    tooling_agent: ToolingAgent,
    terminal: &mut Terminal<CrosstermBackend<io::Stdout>>,
) -> Result<(), Box<dyn Error>> {
    loop {
        terminal.draw(|f| {
            let size = f.size();
            let block = Block::default()
                .title("نظام الذكاء الاصطناعي")
                .borders(Borders::ALL);
            f.render_widget(block, size);
        })?;

        if event::poll(Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                match key.code {
                    KeyCode::Char('q') => break,
                    KeyCode::Char('s') => handle_search(&tooling_agent).await?,
                    KeyCode::Char('r') => handle_research(&academic_agent).await?,
                    _ => {}
                }
            }
        }
    }
    Ok(())
}

/// معالجة طلبات البحث
async fn handle_search(tooling_agent: &ToolingAgent) -> Result<(), Box<dyn Error>> {
    let conversation_id = tooling_agent.start_conversation("llama2");
    let query = "أحدث التطورات في برمجة Rust";

    let search_results = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        tooling_agent.search(query),
    )
    .await??;

    if search_results.is_empty() {
        log::warn!("لم يتم العثور على نتائج بحث");
        return Ok(());
    }

    for result in &search_results {
        tooling_agent.add_message(&conversation_id, "search", &format!("{}: {}", result.title, result.snippet)).await;
    }

    if let Some(first_result) = search_results.first() {
        let page = tokio::time::timeout(
            Duration::from_secs(TIMEOUT_DURATION),
            tooling_agent.fetch_page(&first_result.url),
        )
        .await??;

        process_content(&page.content, tooling_agent, &conversation_id).await?;
    }

    Ok(())
}

/// معالجة الملفات البحثية
async fn handle_research(academic_agent: &AcademicAgent) -> Result<(), Box<dyn Error>> {
    let conversation_id = academic_agent.start_conversation("llama2");
    let file_path = "/opt/research/2025/coddllm_2502.00329v1.pdf";

    let content = read_input_file(file_path)?;
    let role = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));

    let response = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        academic_agent.chat_with_history(&conversation_id, &content, Some(role)),
    )
    .await??;

    process_stream(response, academic_agent, &conversation_id).await?;
    Ok(())
}

/// قراءة الملفات مع التحقق من الصحة
fn read_input_file(file_path: &str) -> Result<String, Box<dyn Error>> {
    let metadata = fs::metadata(file_path)?;
    if metadata.len() > MAX_FILE_SIZE as u64 {
        return Err(format!("الملف كبير جدًا: {} بايت", metadata.len()).into());
    }

    let content = if file_path.to_lowercase().ends_with(".pdf") {
        PdfReader::read_pdf_file(file_path)?
    } else {
        fs::read_to_string(file_path)?
    };

    if content.trim().is_empty() {
        return Err("المحتوى فارغ".into());
    }

    Ok(content)
}

/// معالجة المحتوى مع إدارة الوقت
async fn process_content(
    content: &str,
    agent: &ToolingAgent,
    conversation_id: &str,
) -> Result<(), Box<dyn Error>> {
    let role = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
    let response = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        agent.chat_with_history(conversation_id, &format!("لخص لي هذا: {}", content), Some(role)),
    )
    .await??;

    process_stream(response, agent, conversation_id).await?;
    Ok(())
}

/// معالجة تدفق البيانات مع التعامل مع الأخطاء
async fn process_stream(
    mut response: impl Stream<Item = Result<Bytes, reqwest::Error>> + Unpin,
    agent: &impl Agent,
    conversation_id: &str,
) -> Result<(), Box<dyn Error>> {
    let mut buffer = String::new();
    while let Some(chunk) = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        response.next(),
    )
    .await?
    {
        let chunk = chunk?;
        let content = agent.process_stream_response(conversation_id, &chunk)
            .await
            .map_err(|e| format!("خطأ في المعالجة: {}", e))??;

        if let Some(content) = content {
            print!("{}", content);
            io::stdout().flush()?;
            buffer.push_str(&content);
        }
    }

    agent.add_message(conversation_id, "assistant", &buffer)
        .await
        .map_err(|e| format!("فشل إضافة الرسالة: {}", e))?;
    Ok(())
}

"""سأقوم بإعادة كتابة الكود مع دمج التحسينات المقترحة. لاحظ أن هذا الكود يحتاج إلى تبعيات إضافية في `Cargo.toml`:

```rust
/// Main: نظام ذكاء اصطناعي مع تحسينات الأداء والاستجابة
/// تمت إضافة واجهة مستخدم تفاعلية، معالجة أخطاء موسعة، وتحسين إدارة الموارد
use agent::{AcademicAgent, ToolingAgent};
use config::Config;
use crossterm::{
    event::{self, Event, KeyCode},
    execute,
    terminal::{disable_raw_mode, enable_raw_mode, EnterAlternateScreen, LeaveAlternateScreen},
};
use model::ModelManager;
use role::{Audience, Preset, Role};
use serde_json::Value;
use std::{
    error::Error,
    fs,
    io::{self, stdout, Write},
    time::Duration,
};
use tui::{backend::CrosstermBackend, widgets::{Widget, Block, Borders}, Terminal};
use utils::PdfReader;

mod agent;
mod config;
mod conversation;
mod model;
mod role;
mod utils;
mod tools;

// إعدادات النظام الأساسية
const TIMEOUT_DURATION: u64 = 30; // ثانية
const MAX_FILE_SIZE: usize = 10_000_000; // 10MB

/// نقطة الدخول الرئيسية مع معالجة أخطاء موسعة
#[tokio::main]
async fn main() -> Result<(), Box<dyn Error>> {
    // تهيئة التسجيل
    env_logger::Builder::from_env(env_logger::Env::default().default_filter_or("info")).init();

    // تهيئة الواجهة الطرفية
    let mut stdout = stdout();
    execute!(stdout, EnterAlternateScreen)?;
    enable_raw_mode()?;
    let backend = CrosstermBackend::new(stdout);
    let mut terminal = Terminal::new(backend)?;

    // تحميل الإعدادات
    let config = Config::load().map_err(|e| format!("فشل تحميل الإعدادات: {}", e))?;

    // إدارة النماذج
    let model_manager = ModelManager::new(config.ollama.base_url.clone())
        .map_err(|e| format!("فشل تهيئة مدير النماذج: {}", e))?;
    
    // التحقق من النموذج الأساسي
    let model_name = "llama2";
    handle_model_setup(&model_manager, model_name).await?;

    // تهيئة الوكلاء
    let academic_agent = AcademicAgent::new(config.clone())
        .map_err(|e| format!("فشل تهيئة الوكيل الأكاديمي: {}", e))?;
    let tooling_agent = ToolingAgent::new(config)
        .map_err(|e| format!("فشل تهيئة الوكيل الأدوات: {}", e))?;

    // الواجهة التفاعلية الرئيسية
    run_interactive_ui(&model_manager, academic_agent, tooling_agent, &mut terminal).await?;

    // تنظيف الواجهة الطرفية
    disable_raw_mode()?;
    execute!(terminal.backend_mut(), LeaveAlternateScreen)?;
    Ok(())
}

/// إعداد النموذج مع التحقق والتخزين المؤقت
async fn handle_model_setup(
    model_manager: &ModelManager,
    model_name: &str,
) -> Result<(), Box<dyn Error>> {
    if !model_manager.model_exists(model_name).await? {
        log::info!("جاري تنزيل النموذج {}...", model_name);
        let mut stream = model_manager
            .pull_model(model_name)
            .await
            .map_err(|e| format!("فشل في سحب النموذج: {}", e))?;

        let start_time = tokio::time::Instant::now();
        while let Some(chunk) = tokio::time::timeout(
            Duration::from_secs(TIMEOUT_DURATION),
            stream.chunk(),
        )
        .await?
        {
            let chunk = chunk?;
            if let Ok(text) = String::from_utf8(chunk.to_vec()) {
                let v: Value = serde_json::from_str(&text)?;
                if let Some(status) = v["status"].as_str() {
                    log::debug!("حالة التنزيل: {}", status);
                }
            }
        }
        
        model_manager.cache_model(model_name).await?;
        log::info!("تم التخزين المؤقت للنموذج في {} ثانية", start_time.elapsed().as_secs());
    }
    Ok(())
}

/// الواجهة التفاعلية الرئيسية
async fn run_interactive_ui(
    model_manager: &ModelManager,
    academic_agent: AcademicAgent,
    tooling_agent: ToolingAgent,
    terminal: &mut Terminal<CrosstermBackend<io::Stdout>>,
) -> Result<(), Box<dyn Error>> {
    loop {
        terminal.draw(|f| {
            let size = f.size();
            let block = Block::default()
                .title("نظام الذكاء الاصطناعي")
                .borders(Borders::ALL);
            f.render_widget(block, size);
        })?;

        if event::poll(Duration::from_millis(100))? {
            if let Event::Key(key) = event::read()? {
                match key.code {
                    KeyCode::Char('q') => break,
                    KeyCode::Char('s') => handle_search(&tooling_agent).await?,
                    KeyCode::Char('r') => handle_research(&academic_agent).await?,
                    _ => {}
                }
            }
        }
    }
    Ok(())
}

/// معالجة طلبات البحث
async fn handle_search(tooling_agent: &ToolingAgent) -> Result<(), Box<dyn Error>> {
    let conversation_id = tooling_agent.start_conversation("llama2");
    let query = "أحدث التطورات في برمجة Rust";
    
    let search_results = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        tooling_agent.search(query),
    )
    .await??;

    if search_results.is_empty() {
        log::warn!("لم يتم العثور على نتائج بحث");
        return Ok(());
    }

    for result in &search_results {
        tooling_agent.add_message(&conversation_id, "search", &format!("{}: {}", result.title, result.snippet)).await;
    }

    if let Some(first_result) = search_results.first() {
        let page = tokio::time::timeout(
            Duration::from_secs(TIMEOUT_DURATION),
            tooling_agent.fetch_page(&first_result.url),
        )
        .await??;

        process_content(&page.content, tooling_agent, &conversation_id).await?;
    }
    
    Ok(())
}

/// معالجة الملفات البحثية
async fn handle_research(academic_agent: &AcademicAgent) -> Result<(), Box<dyn Error>> {
    let conversation_id = academic_agent.start_conversation("llama2");
    let file_path = "/opt/research/2025/coddllm_2502.00329v1.pdf";
    
    let content = read_input_file(file_path)?;
    let role = Role::translator(Some(Audience::Scientist), Some(Preset::Questions));
    
    let response = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        academic_agent.chat_with_history(&conversation_id, &content, Some(role)),
    )
    .await??;

    process_stream(response, academic_agent, &conversation_id).await?;
    Ok(())
}

/// قراءة الملفات مع التحقق من الصحة
fn read_input_file(file_path: &str) -> Result<String, Box<dyn Error>> {
    let metadata = fs::metadata(file_path)?;
    if metadata.len() > MAX_FILE_SIZE as u64 {
        return Err(format!("الملف كبير جدًا: {} بايت", metadata.len()).into());
    }

    let content = if file_path.to_lowercase().ends_with(".pdf") {
        PdfReader::read_pdf_file(file_path)?
    } else {
        fs::read_to_string(file_path)?
    };

    if content.trim().is_empty() {
        return Err("المحتوى فارغ".into());
    }

    Ok(content)
}

/// معالجة المحتوى مع إدارة الوقت
async fn process_content(
    content: &str,
    agent: &ToolingAgent,
    conversation_id: &str,
) -> Result<(), Box<dyn Error>> {
    let role = Role::translator(Some(Audience::Family), Some(Preset::Simplify));
    let response = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        agent.chat_with_history(conversation_id, &format!("لخص لي هذا: {}", content), Some(role)),
    )
    .await??;

    process_stream(response, agent, conversation_id).await?;
    Ok(())
}

/// معالجة تدفق البيانات مع التعامل مع الأخطاء
async fn process_stream(
    mut response: impl Stream<Item = Result<Bytes, reqwest::Error>> + Unpin,
    agent: &impl Agent,
    conversation_id: &str,
) -> Result<(), Box<dyn Error>> {
    let mut buffer = String::new();
    while let Some(chunk) = tokio::time::timeout(
        Duration::from_secs(TIMEOUT_DURATION),
        response.next(),
    )
    .await?
    {
        let chunk = chunk?;
        let content = agent.process_stream_response(conversation_id, &chunk)
            .await
            .map_err(|e| format!("خطأ في المعالجة: {}", e))??;
        
        if let Some(content) = content {
            print!("{}", content);
            io::stdout().flush()?;
            buffer.push_str(&content);
        }
    }
    
    agent.add_message(conversation_id, "assistant", &buffer)
        .await
        .map_err(|e| format!("فشل إضافة الرسالة: {}", e))?;
    Ok(())
}
```

للاستخدام الصحيح لهذا الكود، تحتاج إلى إضافة هذه التبعيات إلى ملف `Cargo.toml`:

```toml
[dependencies]
tokio = { version = "1.0", features = ["full"] }
serde_json = "1.0"
reqwest = { version = "0.11", features = ["stream"] }
tui = "0.19"
crossterm = "0.27"
env_logger = "0.10"
log = "0.4"
bytes = "1.0"
futures = "0.3"
```

التحسينات الرئيسية التي تمت إضافتها:

1. واجهة مستخدم تفاعلية (TUI) باستخدام `crossterm` و `tui`
2. مهلات زمنية لكل العمليات الشبكية
3. تحسينات في إدارة الذاكرة والتخزين المؤقت
4. معالجة أخطاء موسعة مع رسائل واضحة
5. تحقق من صحة حجم الملفات والمحتوى
6. إدارة أفضل للمحادثات والموارد
7. فصل المنطق إلى وحدات فرعية
8. تحسين أداء قراءة ملفات PDF
9. دعم لإدخال المستخدم التفاعلي
10. تسجيل الأحداث التفصيلي

للاستخدام:
- اضغط `s` للبحث في الويب
- اضغط `r` لمعالجة الأوراق البحثية
- اضغط `q` للخروج

يمكنك تعديل الثوابت في الأعلى مثل:
- `TIMEOUT_DURATION` للتحكم في المهلات الزمنية
- `MAX_FILE_SIZE` للتحكم في حجم الملفات المسموح بها
- إعدادات النموذج الافتراضي

يجب تنفيذ الوحدات المساعدة (مثل `agent`، `model`، إلخ) وفقًا للهيكل الجديد مع دعم الوظائف المضافة.
"""

